{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Name Hot or Not\n",
    "\n",
    "* Goal: To generate candidates for the best or worst song name using visualizations\n",
    "* Method: Using the names of each song in the db, extract the most frequent bigrams. Show the highest count relationships in a chord diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Chord/Shahin\n",
    "from chord import Chord\n",
    "\n",
    "# hv\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# input: df, column name containing words\n",
    "# output: df with stopwords removed\n",
    "\n",
    "def rm_stopwords(df, name):\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    sw = stopwords.words('english')\n",
    "    realwords = df[~df[name].isin(sw)]\n",
    "    return realwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix for word in each song\n",
    "# input: df with column of words, one per row, multiple duplicates of song title\n",
    "# df 'realwords'\n",
    "# column 'title'\n",
    "\n",
    "def longtable_to_listoflist(realwords):\n",
    "\n",
    "    finallist= []\n",
    "    wordlist = []\n",
    "\n",
    "    uniq_title = realwords.title.unique()\n",
    "\n",
    "    for song in uniq_title:\n",
    "        wordlist = []\n",
    "        print(song)\n",
    "        subset = realwords[realwords['title']==song]\n",
    "        for word in subset['word']:\n",
    "            wordlist.append(word)\n",
    "        finallist.append(wordlist)\n",
    "\n",
    "    print(finallist)\n",
    "    return finallist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate chord chart to html\n",
    "# input: matrix, names, output name\n",
    "# output: html file saved to same directory\n",
    "\n",
    "def gen_html(matrix, names, output):\n",
    "    Chord(matrix, names, padding=0.05, width=1200, wrap_labels=False).to_html(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate matrix for chord/shahin\n",
    "# limit is min co-occur #\n",
    "\n",
    "def gen_chord(inlist, limit):\n",
    "    data = list(itertools.chain.from_iterable(inlist))\n",
    "    matrix, bigram, vocab_index = generate_co_occurrence_matrix(data)\n",
    "\n",
    "    data_matrix = pd.DataFrame(matrix, index=vocab_index, columns=vocab_index)\n",
    "    \n",
    "    data_matrix = rematrix(data_matrix, limit)\n",
    "    \n",
    "    outmatrix = data_matrix.to_numpy()\n",
    "    outmatrix = outmatrix.tolist()\n",
    "#     outmatrix = matrix.tolist()\n",
    "    names = list(data_matrix.index) \n",
    "\n",
    "#     print(matrix)\n",
    "#     print(type(matrix))\n",
    "#     print(data_matrix)\n",
    "#     print(type(names))\n",
    "#     print(names)\n",
    "    return outmatrix, data_matrix, bigram, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build co-occurrence matrix\n",
    "# input: list of lists\n",
    "# output: outmatrix\n",
    "\n",
    "def generate_co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    print('vocab len: ' + str(len(vocab)))\n",
    "    vocab = list(vocab)\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    # Create bigrams from all words in corpus\n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n",
    "    bigram_freqraw = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "    bigram_freq = [bigram for bigram in bigram_freqraw if bigram[1] > 1]\n",
    "\n",
    "    # Initialise co-occurrence matrix\n",
    "    # co_occurrence_matrix[current][previous]\n",
    "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # Loop through the bigrams taking the current and previous word,\n",
    "    # and the number of occurrences of the bigram.\n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "        pos_current = vocab_index[current]\n",
    "        pos_previous = vocab_index[previous]\n",
    "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
    "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
    "\n",
    "    # return the matrix and the index\n",
    "    return co_occurrence_matrix, bigram_freq, vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip cooccurence matrix of low value weights per limit\n",
    "\n",
    "def rematrix(df, limit):\n",
    "    for idx in df.index:\n",
    "        rowsum = df.loc[idx,:].sum(axis=0)\n",
    "        colsum = df[idx].sum(axis=0)\n",
    "#         print('Idx: ' + idx + '; sum of row: ' + str(rowsum))\n",
    "        if rowsum < limit and colsum < limit:\n",
    "            df.drop([idx], axis = 0, inplace=True)\n",
    "            df.drop([idx], axis = 1, inplace=True)\n",
    "#             print('dropping: ' + idx)\n",
    "    print('Final shape: ')\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove parentheses and contents from song titles (or any string input)\n",
    "\n",
    "def Clean_titles(title): \n",
    "    # Search for opening bracket in the name followed by \n",
    "    # any characters repeated any number of times\n",
    "    if re.search('\\(.*', str(title)):\n",
    "        # return the cleaned name \n",
    "        return re.sub(r'\\([^)]*\\)', '', str(title)) # FORCE TO STRING in case of floats\n",
    "    else: \n",
    "        # if clean up needed return the same name \n",
    "        return title "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped = pd.read_csv(r\"../Py code/titles_clean.zip\")\n",
    "deduped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set thresholds\n",
    "\n",
    "hot_thresh = 0.75 # recommend > 0.7\n",
    "not_thresh = 0.25\n",
    "limit = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1739 unique songs with hotness greater than 0.75\n",
      "[\"Let's Get It Started\" 'Low [feat. T-Pain] [Travis Barker Remix]'\n",
      " 'Kite Live from Sydney' ... 'I Just Wanna Know' 'The Calm' 'Solo Dolo ']\n"
     ]
    }
   ],
   "source": [
    "# filter down to hotties\n",
    "hotties = deduped[deduped['artist_hotttnesss'] > hot_thresh]\n",
    "hotties.drop(columns=dropcols, inplace=True)\n",
    "unique_hotties = hotties.title.unique()\n",
    "print(str(len(unique_hotties)) + ' unique songs with hotness greater than ' + str(hot_thresh))\n",
    "print(unique_hotties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1613 unique songs with hotness less than 0.25\n",
      "['Next Time' 'Überlebensgroß' 'Non So Perchè ' ... 'Chills'\n",
      " 'Yo se que te amare ' 'Foggy Mountain Special']\n"
     ]
    }
   ],
   "source": [
    "# filter down to NOTties, doing yourself no favors with naming convention here\n",
    "notties = deduped[(deduped['artist_hotttnesss'] < not_thresh) & (deduped['artist_hotttnesss'] > 0)]\n",
    "notties.drop(columns=dropcols, inplace=True)\n",
    "unique_notties = notties.title.unique()\n",
    "print(str(len(unique_notties)) + ' unique songs with hotness less than ' + str(not_thresh))\n",
    "print(unique_notties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the title data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hotlist: \n",
      "\n",
      "Notlist: \n"
     ]
    }
   ],
   "source": [
    "# create a matrix for word in each song\n",
    "# input: list of song names\n",
    "# output: list of lists, each song name split into component words\n",
    "\n",
    "hotlist = []\n",
    "notlist = []\n",
    "\n",
    "for song in unique_hotties:\n",
    "    subset = song.split()\n",
    "    hotlist.append(subset)\n",
    "    \n",
    "for song in unique_notties:\n",
    "    subset = song.split()\n",
    "    notlist.append(subset)\n",
    "        \n",
    "print('Hotlist: ')\n",
    "# print(hotlist)\n",
    "print('\\nNotlist: ')\n",
    "# print(notlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 2093\n",
      "Final shape: \n",
      "(82, 82)\n"
     ]
    }
   ],
   "source": [
    "# hot_mat, hot_data, hot_bigrams, hot_names - explain each one\n",
    "hot_mat, hot_data, hot_bigrams, hot_names = gen_chord(hotlist, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 2388\n",
      "Final shape: \n",
      "(58, 58)\n"
     ]
    }
   ],
   "source": [
    "not_mat, not_data, not_bigrams, not_names = gen_chord(notlist, limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and save the chord plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'hotties2093_limit4.html'\n",
    "gen_html(hot_mat, hot_names, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'notties2388_limit3.html'\n",
    "gen_html(not_mat, not_names, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hotties\n",
    "\n",
    "'hotties2093_limit4.html'\n",
    "SONG TITLE - potential album art\n",
    "\"Rest The Day, Out All Night\" - Kesha bathtub, Last Friday Night\n",
    "\"Waiting On A Star\" - Twinkle twinkle\n",
    "\"You Ain't Got It\" - No Scrubs\n",
    "\"Where You Go On My Heart\" - celine dion\n",
    "\"The End Of The World\" - it's the end of the world as we know\n",
    "\"Wake Me Up On The Way\" - country roads\n",
    "\"One More 'NO'\"\n",
    "\"You Can't Take Me\"\n",
    "\"Never Too Long\"\n",
    "\"Name My Heart Your Heart\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notties\n",
    "\n",
    "'notties1008_limit3.html'\n",
    "\"Can't I Love Me\"\n",
    "\"Don't You Love Me\"\n",
    "\n",
    "'notties2388_limit3.html'\n",
    "\"I Don't Do It\" - meatloaf\n",
    "\"Man In My Baby\" - nirvana baby swimming\n",
    "\"Life Of A Fool\" - lovefool\n",
    "\"Don't Let Me Down\"\n",
    "\"That Girl That I Can't Be\"\n",
    "\"Now You Are My Man\" - crazy girlfriend face\n",
    "\"Your In My Man\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP HERE\n",
    "### Generate title data from base file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously saved file csv file  \n",
    "# matched.to_csv(r'../words_songs_matched.zip', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file  \n",
    "# matched = pd.read_csv(r\"../words_songs_matched.zip\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "print(matched.shape)\n",
    "matched.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective data drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no longer used, caution INPLACE\n",
    "\n",
    "dropcols = ['track_id', 'song_id', 'artist_id', 'duration']\n",
    "\n",
    "# matched.drop(columns=dropcol, inplace=True)\n",
    "# matched.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract unique songs and save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates against song title\n",
    "deduped = matched.drop_duplicates(subset=\"title\", keep='first', inplace=False)\n",
    "print(deduped.shape)\n",
    "deduped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped['title'] = deduped['title'].apply(Clean_titles)\n",
    "print(deduped.shape)\n",
    "deduped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduped.to_csv(r\"../Py code/titles_clean.zip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Data Review Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_bigrams\n",
    "\n",
    "# for bigram in hot_bigrams:\n",
    "#     current = bigram[0][1]\n",
    "#     print('current: ' + current)\n",
    "#     previous = bigram[0][0]\n",
    "#     print('prev: ' + previous)\n",
    "#     count = bigram[1]\n",
    "#     print('count: ' + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = len(hot_bigrams)\n",
    "for i in range(z):\n",
    "    if hotbigrams[i][1] > 1:\n",
    "        print(hotbigrams[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the trick part, need to rip all the edges which are very light weight\n",
    "\n",
    "print('Hotties len: ' + str(len(hot_names)))\n",
    "print('Hotties len: ' + str(len(np.unique(np.array(hot_names)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_data['Too'].sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with sklearn vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sklearn countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 1)) \n",
    "co_occurrences = bigram_vectorizer.fit_transform(unique_hotties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Printing sparse matrix:', co_occurrences)\n",
    "print('Printing dense matrix', co_occurrences.todense())\n",
    "sum_occ = np.sum(co_occurrences.todense(),axis=0)\n",
    "print('Sum of word-word occurrences:', sum_occ)\n",
    "print('Pretty printig of co_occurrences count:')\n",
    "z = zip(bigram_vectorizer.get_feature_names(),np.array(sum_occ)[0].tolist())\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(z)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
